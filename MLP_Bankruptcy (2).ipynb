{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "MLP_Bankruptcy.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvKa2pdWjdf_"
      },
      "source": [
        "# 1. Keras for Simple Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujf77QG4jdgI"
      },
      "source": [
        "Necessary Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmmzfez0jiDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b76c6d3-a696-4e29-f2c9-4a44ffce4707"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF9qcqr5jdgJ"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYnxy6GmjdgN",
        "outputId": "de1cbba5-10af-44b1-a817-83a58f94ff1f"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wgXLBWMjdgO"
      },
      "source": [
        "Importing the file into **pandas** dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "AhRAG4HOjdgP",
        "outputId": "775521f6-59c1-445d-d523-4f9908d49dbf"
      },
      "source": [
        "df = pd.read_csv(\"Bankruptcy.csv\")\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3f90e806-2e6a-4e63-8309-6026de798749\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NO</th>\n",
              "      <th>D</th>\n",
              "      <th>YR</th>\n",
              "      <th>R1</th>\n",
              "      <th>R2</th>\n",
              "      <th>R3</th>\n",
              "      <th>R4</th>\n",
              "      <th>R5</th>\n",
              "      <th>R6</th>\n",
              "      <th>R7</th>\n",
              "      <th>R8</th>\n",
              "      <th>R9</th>\n",
              "      <th>R10</th>\n",
              "      <th>R11</th>\n",
              "      <th>R12</th>\n",
              "      <th>R13</th>\n",
              "      <th>R14</th>\n",
              "      <th>R15</th>\n",
              "      <th>R16</th>\n",
              "      <th>R17</th>\n",
              "      <th>R18</th>\n",
              "      <th>R19</th>\n",
              "      <th>R20</th>\n",
              "      <th>R21</th>\n",
              "      <th>R22</th>\n",
              "      <th>R23</th>\n",
              "      <th>R24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>78</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.19</td>\n",
              "      <td>10.36</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.23</td>\n",
              "      <td>3.56</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>3.13</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.22</td>\n",
              "      <td>3.78</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.41</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.04</td>\n",
              "      <td>13.29</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.43</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>5.55</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>5.36</td>\n",
              "      <td>1.30</td>\n",
              "      <td>1.12</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>7.74</td>\n",
              "      <td>1.48</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f90e806-2e6a-4e63-8309-6026de798749')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f90e806-2e6a-4e63-8309-6026de798749 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f90e806-2e6a-4e63-8309-6026de798749');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   NO  D  YR    R1    R2    R3    R4  ...   R18    R19   R20   R21   R22   R23   R24\n",
              "0   1  0  78  0.23  0.08  0.02  0.03  ...  0.23   3.56  0.26  1.55  0.43  0.11  0.17\n",
              "1   2  0  77  0.19  0.07  0.09  0.12  ...  0.22   3.78  1.29  1.40  0.06  0.07  0.10\n",
              "2   3  0  72  0.07  0.02  0.03  0.05  ...  0.04  13.29  1.61  1.43  0.03  0.05  0.07\n",
              "3   4  0  80  0.07  0.03  0.04  0.04  ...  0.02   5.36  1.30  1.12 -0.06 -0.08 -0.09\n",
              "4   5  0  81  0.09  0.02  0.03  0.04  ...  0.14   7.74  1.48  1.41  0.03  0.04  0.06\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuWqytM-jdgQ"
      },
      "source": [
        "For the column **Class** which is a response variable, the hot encoding / dummying needs to be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "sqgTp3udjdgR",
        "outputId": "f0484ff8-6018-47ea-9db5-451d9bd06ea1"
      },
      "source": [
        "dum_df = pd.get_dummies(df)\n",
        "\n",
        "dum_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1ad95139-4174-40ce-ad00-6809f440d57d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NO</th>\n",
              "      <th>D</th>\n",
              "      <th>YR</th>\n",
              "      <th>R1</th>\n",
              "      <th>R2</th>\n",
              "      <th>R3</th>\n",
              "      <th>R4</th>\n",
              "      <th>R5</th>\n",
              "      <th>R6</th>\n",
              "      <th>R7</th>\n",
              "      <th>R8</th>\n",
              "      <th>R9</th>\n",
              "      <th>R10</th>\n",
              "      <th>R11</th>\n",
              "      <th>R12</th>\n",
              "      <th>R13</th>\n",
              "      <th>R14</th>\n",
              "      <th>R15</th>\n",
              "      <th>R16</th>\n",
              "      <th>R17</th>\n",
              "      <th>R18</th>\n",
              "      <th>R19</th>\n",
              "      <th>R20</th>\n",
              "      <th>R21</th>\n",
              "      <th>R22</th>\n",
              "      <th>R23</th>\n",
              "      <th>R24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>78</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.19</td>\n",
              "      <td>10.36</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.23</td>\n",
              "      <td>3.56</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>3.13</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.22</td>\n",
              "      <td>3.78</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.41</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.04</td>\n",
              "      <td>13.29</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.43</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>5.55</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>5.36</td>\n",
              "      <td>1.30</td>\n",
              "      <td>1.12</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>7.74</td>\n",
              "      <td>1.48</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ad95139-4174-40ce-ad00-6809f440d57d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ad95139-4174-40ce-ad00-6809f440d57d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ad95139-4174-40ce-ad00-6809f440d57d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   NO  D  YR    R1    R2    R3    R4  ...   R18    R19   R20   R21   R22   R23   R24\n",
              "0   1  0  78  0.23  0.08  0.02  0.03  ...  0.23   3.56  0.26  1.55  0.43  0.11  0.17\n",
              "1   2  0  77  0.19  0.07  0.09  0.12  ...  0.22   3.78  1.29  1.40  0.06  0.07  0.10\n",
              "2   3  0  72  0.07  0.02  0.03  0.05  ...  0.04  13.29  1.61  1.43  0.03  0.05  0.07\n",
              "3   4  0  80  0.07  0.03  0.04  0.04  ...  0.02   5.36  1.30  1.12 -0.06 -0.08 -0.09\n",
              "4   5  0  81  0.09  0.02  0.03  0.04  ...  0.14   7.74  1.48  1.41  0.03  0.04  0.06\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEqrGVrejdgS",
        "outputId": "d094cd94-ee45-4bd0-ffe8-fdfcb87d5ac7"
      },
      "source": [
        "dum_df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(132, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyjvhA9djdgT"
      },
      "source": [
        "We now create two separate objects for feature variables **X** and output variable **y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdEky_IajdgU"
      },
      "source": [
        "X=df.iloc[:,2:]\n",
        "y=df[\"D\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "lUM3pTeVjdgU",
        "outputId": "e73598e3-0a4f-4c80-b795-4a359bf1b094"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b0bb5ad8-3372-409c-ba11-c7b73313e1dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YR</th>\n",
              "      <th>R1</th>\n",
              "      <th>R2</th>\n",
              "      <th>R3</th>\n",
              "      <th>R4</th>\n",
              "      <th>R5</th>\n",
              "      <th>R6</th>\n",
              "      <th>R7</th>\n",
              "      <th>R8</th>\n",
              "      <th>R9</th>\n",
              "      <th>R10</th>\n",
              "      <th>R11</th>\n",
              "      <th>R12</th>\n",
              "      <th>R13</th>\n",
              "      <th>R14</th>\n",
              "      <th>R15</th>\n",
              "      <th>R16</th>\n",
              "      <th>R17</th>\n",
              "      <th>R18</th>\n",
              "      <th>R19</th>\n",
              "      <th>R20</th>\n",
              "      <th>R21</th>\n",
              "      <th>R22</th>\n",
              "      <th>R23</th>\n",
              "      <th>R24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.19</td>\n",
              "      <td>10.36</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.23</td>\n",
              "      <td>3.56</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>77</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>3.13</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.22</td>\n",
              "      <td>3.78</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.41</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.04</td>\n",
              "      <td>13.29</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.43</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>5.55</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>5.36</td>\n",
              "      <td>1.30</td>\n",
              "      <td>1.12</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>7.74</td>\n",
              "      <td>1.48</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0bb5ad8-3372-409c-ba11-c7b73313e1dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0bb5ad8-3372-409c-ba11-c7b73313e1dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0bb5ad8-3372-409c-ba11-c7b73313e1dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   YR    R1    R2    R3    R4    R5  ...    R19   R20   R21   R22   R23   R24\n",
              "0  78  0.23  0.08  0.02  0.03  0.46  ...   3.56  0.26  1.55  0.43  0.11  0.17\n",
              "1  77  0.19  0.07  0.09  0.12  0.02  ...   3.78  1.29  1.40  0.06  0.07  0.10\n",
              "2  72  0.07  0.02  0.03  0.05  0.06  ...  13.29  1.61  1.43  0.03  0.05  0.07\n",
              "3  80  0.07  0.03  0.04  0.04  0.04  ...   5.36  1.30  1.12 -0.06 -0.08 -0.09\n",
              "4  81  0.09  0.02  0.03  0.04  0.06  ...   7.74  1.48  1.41  0.03  0.04  0.06\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iavdHM2DjdgV",
        "outputId": "2158a7be-2fd8-4371-d376-ba9a79e75944"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: D, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtEMz7IKjdgX"
      },
      "source": [
        "We now, split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RADzEKkGjdgX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n",
        "                                                    random_state=2019,stratify=y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4xpWtPUjdgW"
      },
      "source": [
        "Scaling the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WI-wuVWjdgW"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)    \n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix1ZTq5sjdgY",
        "outputId": "d293c4ea-14d9-41fa-8bab-fb41ccc0ab10"
      },
      "source": [
        "(X_train.shape, y_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((92, 25), (92,))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXLXAxBajdgY"
      },
      "source": [
        "Conversion of objects **y_train** and **y_test** into numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Qj2rtyjdgZ"
      },
      "source": [
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc7veqx4jdgZ"
      },
      "source": [
        "Let us now define the neural network through which we plan to build the MLP model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QjSRmKHjdgZ"
      },
      "source": [
        "For getting reproducible results, we set random number seed and do necessary imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nsMbqhojdga"
      },
      "source": [
        "**Model Definition:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwDSbKZujdga"
      },
      "source": [
        "tf.random.set_seed(2021)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(15, activation='relu',input_shape=(X_train.shape[1], )), \n",
        "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(3, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqpa-dKsjdga"
      },
      "source": [
        "**Initialized Variable Values:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G7P06usjdgb",
        "outputId": "0f389574-23c2-4367-8309-715e8615dbe5"
      },
      "source": [
        "model.variables"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense/kernel:0' shape=(25, 15) dtype=float32, numpy=\n",
              " array([[ 0.0566965 , -0.17881314, -0.10430849, -0.05096638,  0.0944474 ,\n",
              "         -0.09609807,  0.13994241,  0.3669867 , -0.18670084, -0.12987858,\n",
              "          0.37843353,  0.04249936,  0.07837707, -0.33141175, -0.09640214],\n",
              "        [ 0.14090264,  0.36561048,  0.2544241 , -0.14130951,  0.36163086,\n",
              "         -0.28051203,  0.38369018,  0.37835348, -0.12794435,  0.3158068 ,\n",
              "         -0.15545504, -0.00749749,  0.07595614,  0.07862777, -0.18393242],\n",
              "        [ 0.2884218 , -0.06241634,  0.00969228,  0.15163702,  0.27476966,\n",
              "         -0.32946727,  0.08879322,  0.13526952, -0.38276476,  0.13391566,\n",
              "          0.09383178,  0.05614209,  0.04707089,  0.35698342,  0.05875307],\n",
              "        [-0.32111067,  0.17276454, -0.33647516,  0.11923426,  0.12474644,\n",
              "          0.28343254, -0.31499153, -0.1976699 , -0.04513997,  0.20653576,\n",
              "          0.08638677, -0.32285228,  0.13421762,  0.33912736, -0.05462912],\n",
              "        [-0.37593168,  0.06780878,  0.2876464 ,  0.32468754, -0.08694431,\n",
              "          0.09254152,  0.18383461, -0.0031136 , -0.30725685, -0.3295018 ,\n",
              "          0.310682  , -0.29338735, -0.17359708,  0.2090534 ,  0.11720163],\n",
              "        [-0.21196371,  0.20535696, -0.25443333,  0.33580136,  0.13468325,\n",
              "         -0.2391467 , -0.21916413,  0.06667301,  0.36723757, -0.2856143 ,\n",
              "         -0.3655054 , -0.2933925 , -0.10466427, -0.3692296 ,  0.05191535],\n",
              "        [ 0.3179888 ,  0.37377214,  0.11270279, -0.21383229,  0.32044935,\n",
              "         -0.16279137, -0.22621709, -0.33329463,  0.23352844, -0.32994154,\n",
              "          0.10136831, -0.11142626, -0.3133225 , -0.01029572,  0.3104673 ],\n",
              "        [ 0.3506627 , -0.03099465, -0.21939102,  0.16899794,  0.14647776,\n",
              "         -0.10872063, -0.00159636,  0.19776934,  0.36017066,  0.27573848,\n",
              "         -0.00364047, -0.28490922, -0.04060262,  0.18931377, -0.3165869 ],\n",
              "        [-0.31146115, -0.02169868,  0.37437302,  0.0321584 , -0.36773428,\n",
              "          0.3079303 , -0.31031936, -0.35188895, -0.32444644, -0.17001432,\n",
              "          0.3600822 , -0.3400451 , -0.31362963,  0.01841465, -0.24336095],\n",
              "        [-0.2864138 , -0.37429598, -0.34646055, -0.23986176, -0.3274432 ,\n",
              "          0.02557313,  0.01594052, -0.11414167,  0.29533   ,  0.10599211,\n",
              "          0.23796588, -0.05869842, -0.3677822 , -0.02125758,  0.33179313],\n",
              "        [-0.36789098,  0.3138216 ,  0.27691245, -0.3135987 , -0.3825909 ,\n",
              "         -0.2593576 , -0.0627799 ,  0.35435098, -0.28513435,  0.14419878,\n",
              "         -0.20796931,  0.35974395, -0.19343632,  0.25365567, -0.33155137],\n",
              "        [ 0.2018705 ,  0.25464034,  0.16421664,  0.36686677, -0.18190198,\n",
              "         -0.34101215, -0.21588582,  0.08810094, -0.2280576 , -0.37122026,\n",
              "          0.20135319, -0.3374042 , -0.35374287,  0.06694439,  0.3712725 ],\n",
              "        [-0.2230432 ,  0.02140293,  0.30376405, -0.10457894,  0.26429415,\n",
              "          0.11602169, -0.1986534 , -0.08184108, -0.3759095 ,  0.10446677,\n",
              "         -0.3150816 , -0.0703145 ,  0.30210674, -0.00293362,  0.06077787],\n",
              "        [-0.19521284, -0.00728363, -0.05304208,  0.2808681 , -0.31977102,\n",
              "          0.18428266, -0.10002995, -0.2517048 ,  0.2622854 ,  0.0217573 ,\n",
              "          0.09538889, -0.27258682,  0.29785573, -0.04650161, -0.02234578],\n",
              "        [-0.27371782, -0.31148562, -0.32539946,  0.02479351,  0.3758505 ,\n",
              "          0.11998057, -0.03355825,  0.1238119 , -0.22849463, -0.27978745,\n",
              "         -0.07778907,  0.20554161,  0.0974994 ,  0.20109409,  0.13352281],\n",
              "        [ 0.16500288,  0.17886132, -0.16079555, -0.3528685 ,  0.22663057,\n",
              "          0.33860338,  0.20178926,  0.14787036, -0.28452492, -0.1322273 ,\n",
              "          0.06106931, -0.22628902,  0.08708963, -0.18927091, -0.17396487],\n",
              "        [ 0.38571888,  0.197083  , -0.23067467,  0.03839672, -0.08470112,\n",
              "          0.37105286, -0.13009077,  0.37118387,  0.36800438,  0.06965694,\n",
              "          0.04828146, -0.38535434,  0.13029373,  0.01113269, -0.07803127],\n",
              "        [ 0.06164283, -0.37659198,  0.14042425, -0.13551366,  0.03992605,\n",
              "          0.36283475, -0.24038275, -0.36712077, -0.00254017, -0.14971754,\n",
              "          0.0505347 , -0.3394711 ,  0.38719815, -0.31208277,  0.23393255],\n",
              "        [-0.03970915, -0.16943453,  0.05885473, -0.37098792, -0.3792734 ,\n",
              "         -0.32658142, -0.20769617, -0.36058584, -0.1486621 , -0.22986725,\n",
              "         -0.2880348 , -0.12239051, -0.17443764,  0.38312316,  0.36905992],\n",
              "        [ 0.10156241, -0.22038975, -0.3610434 , -0.2911781 , -0.32302698,\n",
              "         -0.04229066, -0.33137694, -0.10526919, -0.11175683, -0.02677566,\n",
              "         -0.13461798,  0.17803001, -0.2013316 , -0.3261199 ,  0.33628154],\n",
              "        [ 0.21714145,  0.02447799,  0.37838525, -0.04852799,  0.0979152 ,\n",
              "         -0.19934115, -0.37801355, -0.21997109,  0.04936773,  0.3850944 ,\n",
              "          0.04524598, -0.11646309, -0.26853502, -0.3459729 ,  0.21351707],\n",
              "        [ 0.36596376, -0.1826092 , -0.18866268, -0.3220785 , -0.31993437,\n",
              "         -0.28514624,  0.32765734, -0.09670445,  0.01905122, -0.13243192,\n",
              "          0.24633908, -0.25069296, -0.31964794,  0.03371716, -0.33263314],\n",
              "        [-0.16336675, -0.06123275,  0.03793079,  0.11587805,  0.14674681,\n",
              "         -0.20087056,  0.33688635, -0.09567532,  0.10756761, -0.30041572,\n",
              "          0.09081674,  0.33254373,  0.14987797,  0.20172006, -0.1653198 ],\n",
              "        [-0.06493047, -0.14993426,  0.33700305,  0.04337722, -0.10279199,\n",
              "          0.2655545 ,  0.25766546, -0.02380696,  0.38556385, -0.35366142,\n",
              "         -0.19706637, -0.3425437 ,  0.04918396,  0.33131778, -0.37638164],\n",
              "        [ 0.35330093,  0.15425372,  0.17027134, -0.3056156 ,  0.25610375,\n",
              "         -0.31662688,  0.37755215,  0.15169662,  0.02340493, -0.23563826,\n",
              "         -0.18072326, -0.19701567, -0.21391077,  0.17103541, -0.20658274]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense/bias:0' shape=(15,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_1/kernel:0' shape=(15, 8) dtype=float32, numpy=\n",
              " array([[ 0.01406407,  0.4299385 , -0.1014919 ,  0.12345904, -0.19594616,\n",
              "         -0.27731013, -0.31431508, -0.46497208],\n",
              "        [-0.17696682,  0.5021685 ,  0.43236303,  0.34569907,  0.29682153,\n",
              "          0.37888336,  0.3574496 , -0.07706016],\n",
              "        [ 0.28552884, -0.35243827, -0.24957302,  0.40136355, -0.27574548,\n",
              "         -0.12677556, -0.12173888, -0.22775137],\n",
              "        [ 0.1655075 ,  0.18769968,  0.28145164,  0.08484745, -0.2989998 ,\n",
              "          0.05076921,  0.3448534 , -0.15053216],\n",
              "        [-0.28223673, -0.0201084 ,  0.16107494,  0.15609974, -0.4993081 ,\n",
              "         -0.12901726, -0.4599504 ,  0.22262907],\n",
              "        [ 0.12844175, -0.46343052, -0.43347347, -0.38925666,  0.3802616 ,\n",
              "          0.05906963,  0.39823014, -0.06117496],\n",
              "        [-0.44984564, -0.34927267,  0.28299767, -0.44230142, -0.03901628,\n",
              "         -0.43165565, -0.45899993,  0.29273933],\n",
              "        [-0.36189786,  0.17431486, -0.27977556, -0.1486795 , -0.05831012,\n",
              "         -0.3341701 , -0.19795164, -0.22259599],\n",
              "        [-0.4553079 ,  0.38119102,  0.12056351, -0.47916487, -0.2772166 ,\n",
              "         -0.11552981,  0.28816378, -0.10756791],\n",
              "        [ 0.0645414 , -0.46520537,  0.00852472,  0.21081394, -0.25323328,\n",
              "          0.20926851, -0.30528852,  0.4076271 ],\n",
              "        [ 0.45358723,  0.29325914,  0.06264186, -0.24025857, -0.15848249,\n",
              "         -0.3471141 , -0.28002948, -0.41972747],\n",
              "        [-0.07744679,  0.48465246, -0.2484208 ,  0.30543292, -0.42438394,\n",
              "         -0.16355923,  0.22393245,  0.28014177],\n",
              "        [ 0.5072848 , -0.24215046,  0.33978724,  0.15446275, -0.25434786,\n",
              "          0.17625177,  0.11556792,  0.4744581 ],\n",
              "        [ 0.31343186, -0.11547559, -0.21652827,  0.40761787, -0.21235156,\n",
              "          0.48424733, -0.2633775 , -0.21392244],\n",
              "        [-0.4791511 , -0.05404577,  0.32666534, -0.07208791, -0.09112534,\n",
              "         -0.3965729 ,  0.19131595,  0.11501569]], dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_2/kernel:0' shape=(8, 3) dtype=float32, numpy=\n",
              " array([[ 0.41299778, -0.21309447,  0.40112597],\n",
              "        [ 0.22151816,  0.5618339 , -0.6619123 ],\n",
              "        [-0.6802294 ,  0.6210975 , -0.25344267],\n",
              "        [ 0.68611294, -0.36735693,  0.048154  ],\n",
              "        [ 0.38786715, -0.617959  , -0.449952  ],\n",
              "        [-0.5054895 ,  0.24533898, -0.21215475],\n",
              "        [ 0.49412805,  0.47265726, -0.45763507],\n",
              "        [-0.3827514 , -0.7267497 , -0.7144897 ]], dtype=float32)>,\n",
              " <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_3/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
              " array([[ 0.11942446],\n",
              "        [ 0.43881524],\n",
              "        [-0.85673285]], dtype=float32)>,\n",
              " <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJmecj6kjdgc"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bUMljUFjdgc",
        "outputId": "72aa4d4b-617d-46ba-88fc-114cb8c45ac6"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 15)                390       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 128       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 27        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 549\n",
            "Trainable params: 549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRie3U4kjdgc"
      },
      "source": [
        "**Model Fitting:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcS2vR2Ijdgd",
        "outputId": "a6ecf47c-d720-4267-d403-a8bca2be831a"
      },
      "source": [
        "history = model.fit( X_train,y_train,validation_data=(X_test,y_test),verbose=2,epochs=500)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 - 2s - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000 - 2s/epoch - 707ms/step\n",
            "Epoch 2/500\n",
            "3/3 - 0s - loss: 0.6960 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 3/500\n",
            "3/3 - 0s - loss: 0.6960 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000 - 32ms/epoch - 11ms/step\n",
            "Epoch 4/500\n",
            "3/3 - 0s - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 31ms/epoch - 10ms/step\n",
            "Epoch 5/500\n",
            "3/3 - 0s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 33ms/epoch - 11ms/step\n",
            "Epoch 6/500\n",
            "3/3 - 0s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 32ms/epoch - 11ms/step\n",
            "Epoch 7/500\n",
            "3/3 - 0s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 39ms/epoch - 13ms/step\n",
            "Epoch 8/500\n",
            "3/3 - 0s - loss: 0.6958 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 38ms/epoch - 13ms/step\n",
            "Epoch 9/500\n",
            "3/3 - 0s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 34ms/epoch - 11ms/step\n",
            "Epoch 10/500\n",
            "3/3 - 0s - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 11/500\n",
            "3/3 - 0s - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000 - 38ms/epoch - 13ms/step\n",
            "Epoch 12/500\n",
            "3/3 - 0s - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 13/500\n",
            "3/3 - 0s - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6929 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 14/500\n",
            "3/3 - 0s - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6929 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 15/500\n",
            "3/3 - 0s - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6929 - val_accuracy: 0.5000 - 40ms/epoch - 13ms/step\n",
            "Epoch 16/500\n",
            "3/3 - 0s - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.5000 - 56ms/epoch - 19ms/step\n",
            "Epoch 17/500\n",
            "3/3 - 0s - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.5000 - 35ms/epoch - 12ms/step\n",
            "Epoch 18/500\n",
            "3/3 - 0s - loss: 0.6955 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.5000 - 38ms/epoch - 13ms/step\n",
            "Epoch 19/500\n",
            "3/3 - 0s - loss: 0.6955 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.5000 - 39ms/epoch - 13ms/step\n",
            "Epoch 20/500\n",
            "3/3 - 0s - loss: 0.6955 - accuracy: 0.5000 - val_loss: 0.6927 - val_accuracy: 0.5000 - 48ms/epoch - 16ms/step\n",
            "Epoch 21/500\n",
            "3/3 - 0s - loss: 0.6954 - accuracy: 0.5000 - val_loss: 0.6927 - val_accuracy: 0.5000 - 39ms/epoch - 13ms/step\n",
            "Epoch 22/500\n",
            "3/3 - 0s - loss: 0.6954 - accuracy: 0.5000 - val_loss: 0.6927 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 23/500\n",
            "3/3 - 0s - loss: 0.6954 - accuracy: 0.5000 - val_loss: 0.6927 - val_accuracy: 0.5000 - 38ms/epoch - 13ms/step\n",
            "Epoch 24/500\n",
            "3/3 - 0s - loss: 0.6954 - accuracy: 0.5000 - val_loss: 0.6926 - val_accuracy: 0.5000 - 37ms/epoch - 12ms/step\n",
            "Epoch 25/500\n",
            "3/3 - 0s - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6926 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 26/500\n",
            "3/3 - 0s - loss: 0.6954 - accuracy: 0.5000 - val_loss: 0.6926 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 27/500\n",
            "3/3 - 0s - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6926 - val_accuracy: 0.5250 - 32ms/epoch - 11ms/step\n",
            "Epoch 28/500\n",
            "3/3 - 0s - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 29/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 30/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 31/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5250 - 32ms/epoch - 11ms/step\n",
            "Epoch 32/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 33/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 34/500\n",
            "3/3 - 0s - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 35/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 36/500\n",
            "3/3 - 0s - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 37/500\n",
            "3/3 - 0s - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 38/500\n",
            "3/3 - 0s - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 39/500\n",
            "3/3 - 0s - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 40/500\n",
            "3/3 - 0s - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6923 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 41/500\n",
            "3/3 - 0s - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6923 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 42/500\n",
            "3/3 - 0s - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6923 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 43/500\n",
            "3/3 - 0s - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6923 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 44/500\n",
            "3/3 - 0s - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6923 - val_accuracy: 0.5250 - 46ms/epoch - 15ms/step\n",
            "Epoch 45/500\n",
            "3/3 - 0s - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6923 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 46/500\n",
            "3/3 - 0s - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 47/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 48/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5500 - 42ms/epoch - 14ms/step\n",
            "Epoch 49/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 50/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 51/500\n",
            "3/3 - 0s - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 52/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 53/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 54/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 55/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 56/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 57/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 42ms/epoch - 14ms/step\n",
            "Epoch 58/500\n",
            "3/3 - 0s - loss: 0.6949 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 59/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 60/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 61/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5250 - 44ms/epoch - 15ms/step\n",
            "Epoch 62/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 63/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 64/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 65/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 66/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 67/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 41ms/epoch - 14ms/step\n",
            "Epoch 68/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 44ms/epoch - 15ms/step\n",
            "Epoch 69/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 70/500\n",
            "3/3 - 0s - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 71/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 72/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 73/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 74/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 75/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 76/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 77/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 78/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 79/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 80/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 81/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 82/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 83/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 84/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 85/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 86/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 87/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 88/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 42ms/epoch - 14ms/step\n",
            "Epoch 89/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 90/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 46ms/epoch - 15ms/step\n",
            "Epoch 91/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 46ms/epoch - 15ms/step\n",
            "Epoch 92/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 93/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 94/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.4891 - val_loss: 0.6918 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 95/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 96/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 97/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4891 - val_loss: 0.6918 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 98/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 99/500\n",
            "3/3 - 0s - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 100/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 101/500\n",
            "3/3 - 0s - loss: 0.6947 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 102/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 103/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 104/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 105/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 54ms/epoch - 18ms/step\n",
            "Epoch 106/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 107/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 108/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5250 - 41ms/epoch - 14ms/step\n",
            "Epoch 109/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 110/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5250 - 42ms/epoch - 14ms/step\n",
            "Epoch 111/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 112/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 113/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 114/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4891 - val_loss: 0.6917 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 115/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 49ms/epoch - 16ms/step\n",
            "Epoch 116/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 117/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 118/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4674 - val_loss: 0.6917 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 119/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 120/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 121/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4783 - val_loss: 0.6917 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 122/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 46ms/epoch - 15ms/step\n",
            "Epoch 123/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 124/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 54ms/epoch - 18ms/step\n",
            "Epoch 125/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 126/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 127/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 128/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 129/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4674 - val_loss: 0.6916 - val_accuracy: 0.5250 - 44ms/epoch - 15ms/step\n",
            "Epoch 130/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4674 - val_loss: 0.6916 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 131/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 132/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 133/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 51ms/epoch - 17ms/step\n",
            "Epoch 134/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 135/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5250 - 44ms/epoch - 15ms/step\n",
            "Epoch 136/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 137/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 42ms/epoch - 14ms/step\n",
            "Epoch 138/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 139/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 140/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 141/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 142/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6916 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 143/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5250 - 42ms/epoch - 14ms/step\n",
            "Epoch 144/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 37ms/epoch - 12ms/step\n",
            "Epoch 145/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6916 - val_accuracy: 0.5250 - 34ms/epoch - 11ms/step\n",
            "Epoch 146/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5250 - 39ms/epoch - 13ms/step\n",
            "Epoch 147/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4783 - val_loss: 0.6916 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 148/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 149/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6915 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 150/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4891 - val_loss: 0.6915 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 151/500\n",
            "3/3 - 0s - loss: 0.6945 - accuracy: 0.4783 - val_loss: 0.6915 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 152/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6915 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 153/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4239 - val_loss: 0.6915 - val_accuracy: 0.5500 - 40ms/epoch - 13ms/step\n",
            "Epoch 154/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4674 - val_loss: 0.6915 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 155/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4565 - val_loss: 0.6915 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 156/500\n",
            "3/3 - 0s - loss: 0.6944 - accuracy: 0.4022 - val_loss: 0.6915 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 157/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4674 - val_loss: 0.6915 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 158/500\n",
            "3/3 - 0s - loss: 0.6943 - accuracy: 0.4348 - val_loss: 0.6915 - val_accuracy: 0.5500 - 41ms/epoch - 14ms/step\n",
            "Epoch 159/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4457 - val_loss: 0.6915 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 160/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4891 - val_loss: 0.6915 - val_accuracy: 0.5750 - 58ms/epoch - 19ms/step\n",
            "Epoch 161/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4457 - val_loss: 0.6915 - val_accuracy: 0.5750 - 41ms/epoch - 14ms/step\n",
            "Epoch 162/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4348 - val_loss: 0.6915 - val_accuracy: 0.5750 - 32ms/epoch - 11ms/step\n",
            "Epoch 163/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4565 - val_loss: 0.6915 - val_accuracy: 0.5750 - 54ms/epoch - 18ms/step\n",
            "Epoch 164/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4457 - val_loss: 0.6915 - val_accuracy: 0.5750 - 33ms/epoch - 11ms/step\n",
            "Epoch 165/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4674 - val_loss: 0.6915 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 166/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4348 - val_loss: 0.6915 - val_accuracy: 0.5750 - 44ms/epoch - 15ms/step\n",
            "Epoch 167/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4022 - val_loss: 0.6915 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 168/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4348 - val_loss: 0.6915 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 169/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4348 - val_loss: 0.6915 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 170/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4348 - val_loss: 0.6915 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 171/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4130 - val_loss: 0.6915 - val_accuracy: 0.5750 - 51ms/epoch - 17ms/step\n",
            "Epoch 172/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4239 - val_loss: 0.6915 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 173/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4457 - val_loss: 0.6914 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 174/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4239 - val_loss: 0.6914 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 175/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5750 - 39ms/epoch - 13ms/step\n",
            "Epoch 176/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 177/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 178/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4022 - val_loss: 0.6914 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 179/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4022 - val_loss: 0.6914 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 180/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4022 - val_loss: 0.6914 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 181/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4022 - val_loss: 0.6914 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 182/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 183/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 184/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 41ms/epoch - 14ms/step\n",
            "Epoch 185/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 186/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 187/500\n",
            "3/3 - 0s - loss: 0.6942 - accuracy: 0.4239 - val_loss: 0.6914 - val_accuracy: 0.5750 - 42ms/epoch - 14ms/step\n",
            "Epoch 188/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5750 - 42ms/epoch - 14ms/step\n",
            "Epoch 189/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6914 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 190/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4022 - val_loss: 0.6914 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 191/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 192/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 193/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6914 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 194/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4130 - val_loss: 0.6914 - val_accuracy: 0.5500 - 41ms/epoch - 14ms/step\n",
            "Epoch 195/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6914 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 196/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4348 - val_loss: 0.6914 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 197/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6914 - val_accuracy: 0.5500 - 45ms/epoch - 15ms/step\n",
            "Epoch 198/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6914 - val_accuracy: 0.5500 - 42ms/epoch - 14ms/step\n",
            "Epoch 199/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.3913 - val_loss: 0.6913 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 200/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 201/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 202/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 52ms/epoch - 17ms/step\n",
            "Epoch 203/500\n",
            "3/3 - 0s - loss: 0.6941 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 44ms/epoch - 15ms/step\n",
            "Epoch 204/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 205/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 206/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 44ms/epoch - 15ms/step\n",
            "Epoch 207/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 208/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 209/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 40ms/epoch - 13ms/step\n",
            "Epoch 210/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 211/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 212/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 34ms/epoch - 11ms/step\n",
            "Epoch 213/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 34ms/epoch - 11ms/step\n",
            "Epoch 214/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 215/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 216/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 217/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5500 - 41ms/epoch - 14ms/step\n",
            "Epoch 218/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 45ms/epoch - 15ms/step\n",
            "Epoch 219/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4348 - val_loss: 0.6913 - val_accuracy: 0.5500 - 41ms/epoch - 14ms/step\n",
            "Epoch 220/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 221/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 222/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 223/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 224/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.6000 - 34ms/epoch - 11ms/step\n",
            "Epoch 225/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5750 - 41ms/epoch - 14ms/step\n",
            "Epoch 226/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 227/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 228/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6913 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 229/500\n",
            "3/3 - 0s - loss: 0.6940 - accuracy: 0.3913 - val_loss: 0.6913 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 230/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6913 - val_accuracy: 0.5750 - 42ms/epoch - 14ms/step\n",
            "Epoch 231/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 232/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6913 - val_accuracy: 0.5750 - 39ms/epoch - 13ms/step\n",
            "Epoch 233/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 234/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 235/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 236/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 237/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 238/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5750 - 46ms/epoch - 15ms/step\n",
            "Epoch 239/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 240/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 241/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 242/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.3913 - val_loss: 0.6912 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 243/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 244/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 245/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5500 - 42ms/epoch - 14ms/step\n",
            "Epoch 246/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.3804 - val_loss: 0.6912 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 247/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 248/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 249/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.3913 - val_loss: 0.6912 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 250/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5250 - 36ms/epoch - 12ms/step\n",
            "Epoch 251/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5500 - 50ms/epoch - 17ms/step\n",
            "Epoch 252/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5500 - 57ms/epoch - 19ms/step\n",
            "Epoch 253/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 254/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5500 - 40ms/epoch - 13ms/step\n",
            "Epoch 255/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 256/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6912 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 257/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4239 - val_loss: 0.6912 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 258/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 259/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5250 - 35ms/epoch - 12ms/step\n",
            "Epoch 260/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5250 - 40ms/epoch - 13ms/step\n",
            "Epoch 261/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5250 - 38ms/epoch - 13ms/step\n",
            "Epoch 262/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 263/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5500 - 44ms/epoch - 15ms/step\n",
            "Epoch 264/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4022 - val_loss: 0.6912 - val_accuracy: 0.5500 - 54ms/epoch - 18ms/step\n",
            "Epoch 265/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5500 - 42ms/epoch - 14ms/step\n",
            "Epoch 266/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4130 - val_loss: 0.6912 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 267/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4348 - val_loss: 0.6912 - val_accuracy: 0.5750 - 31ms/epoch - 10ms/step\n",
            "Epoch 268/500\n",
            "3/3 - 0s - loss: 0.6938 - accuracy: 0.4130 - val_loss: 0.6911 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 269/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 270/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 271/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 272/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 42ms/epoch - 14ms/step\n",
            "Epoch 273/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4239 - val_loss: 0.6911 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 274/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4130 - val_loss: 0.6911 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 275/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4239 - val_loss: 0.6911 - val_accuracy: 0.5750 - 44ms/epoch - 15ms/step\n",
            "Epoch 276/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4239 - val_loss: 0.6911 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 277/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 41ms/epoch - 14ms/step\n",
            "Epoch 278/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 34ms/epoch - 11ms/step\n",
            "Epoch 279/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 39ms/epoch - 13ms/step\n",
            "Epoch 280/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 281/500\n",
            "3/3 - 0s - loss: 0.6939 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 282/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 283/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4239 - val_loss: 0.6911 - val_accuracy: 0.5750 - 43ms/epoch - 14ms/step\n",
            "Epoch 284/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4130 - val_loss: 0.6911 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 285/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4239 - val_loss: 0.6911 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 286/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4348 - val_loss: 0.6911 - val_accuracy: 0.5750 - 56ms/epoch - 19ms/step\n",
            "Epoch 287/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 288/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4565 - val_loss: 0.6911 - val_accuracy: 0.5750 - 34ms/epoch - 11ms/step\n",
            "Epoch 289/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4674 - val_loss: 0.6911 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 290/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 291/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4239 - val_loss: 0.6911 - val_accuracy: 0.5750 - 38ms/epoch - 13ms/step\n",
            "Epoch 292/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 293/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6911 - val_accuracy: 0.5750 - 34ms/epoch - 11ms/step\n",
            "Epoch 294/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4130 - val_loss: 0.6910 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 295/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4674 - val_loss: 0.6910 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 296/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 297/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4130 - val_loss: 0.6910 - val_accuracy: 0.5750 - 46ms/epoch - 15ms/step\n",
            "Epoch 298/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5750 - 39ms/epoch - 13ms/step\n",
            "Epoch 299/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4565 - val_loss: 0.6910 - val_accuracy: 0.5500 - 34ms/epoch - 11ms/step\n",
            "Epoch 300/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4674 - val_loss: 0.6910 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 301/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4783 - val_loss: 0.6910 - val_accuracy: 0.5750 - 34ms/epoch - 11ms/step\n",
            "Epoch 302/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4783 - val_loss: 0.6910 - val_accuracy: 0.5750 - 34ms/epoch - 11ms/step\n",
            "Epoch 303/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4674 - val_loss: 0.6910 - val_accuracy: 0.5750 - 32ms/epoch - 11ms/step\n",
            "Epoch 304/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4565 - val_loss: 0.6910 - val_accuracy: 0.5500 - 32ms/epoch - 11ms/step\n",
            "Epoch 305/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4783 - val_loss: 0.6910 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 306/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 307/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4565 - val_loss: 0.6910 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 308/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4783 - val_loss: 0.6910 - val_accuracy: 0.5500 - 57ms/epoch - 19ms/step\n",
            "Epoch 309/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4891 - val_loss: 0.6910 - val_accuracy: 0.5750 - 56ms/epoch - 19ms/step\n",
            "Epoch 310/500\n",
            "3/3 - 0s - loss: 0.6937 - accuracy: 0.4674 - val_loss: 0.6910 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 311/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5750 - 39ms/epoch - 13ms/step\n",
            "Epoch 312/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4674 - val_loss: 0.6910 - val_accuracy: 0.5750 - 33ms/epoch - 11ms/step\n",
            "Epoch 313/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5750 - 33ms/epoch - 11ms/step\n",
            "Epoch 314/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4783 - val_loss: 0.6910 - val_accuracy: 0.5750 - 39ms/epoch - 13ms/step\n",
            "Epoch 315/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4674 - val_loss: 0.6910 - val_accuracy: 0.5750 - 46ms/epoch - 15ms/step\n",
            "Epoch 316/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 317/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4783 - val_loss: 0.6910 - val_accuracy: 0.5750 - 34ms/epoch - 11ms/step\n",
            "Epoch 318/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4457 - val_loss: 0.6910 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 319/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4674 - val_loss: 0.6909 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 320/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4891 - val_loss: 0.6909 - val_accuracy: 0.5500 - 52ms/epoch - 17ms/step\n",
            "Epoch 321/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4565 - val_loss: 0.6909 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 322/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6909 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 323/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4565 - val_loss: 0.6909 - val_accuracy: 0.5500 - 33ms/epoch - 11ms/step\n",
            "Epoch 324/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4348 - val_loss: 0.6909 - val_accuracy: 0.5500 - 37ms/epoch - 12ms/step\n",
            "Epoch 325/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 326/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4457 - val_loss: 0.6909 - val_accuracy: 0.5500 - 36ms/epoch - 12ms/step\n",
            "Epoch 327/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.5500 - 35ms/epoch - 12ms/step\n",
            "Epoch 328/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4674 - val_loss: 0.6909 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 329/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4674 - val_loss: 0.6909 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 330/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 45ms/epoch - 15ms/step\n",
            "Epoch 331/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 34ms/epoch - 11ms/step\n",
            "Epoch 332/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 333/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 334/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 41ms/epoch - 14ms/step\n",
            "Epoch 335/500\n",
            "3/3 - 0s - loss: 0.6936 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.5750 - 44ms/epoch - 15ms/step\n",
            "Epoch 336/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.5500 - 39ms/epoch - 13ms/step\n",
            "Epoch 337/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.5500 - 38ms/epoch - 13ms/step\n",
            "Epoch 338/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4457 - val_loss: 0.6909 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 339/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6909 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 340/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4891 - val_loss: 0.6909 - val_accuracy: 0.6000 - 41ms/epoch - 14ms/step\n",
            "Epoch 341/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4891 - val_loss: 0.6909 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 342/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 343/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 45ms/epoch - 15ms/step\n",
            "Epoch 344/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.6000 - 45ms/epoch - 15ms/step\n",
            "Epoch 345/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6909 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 346/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 347/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4674 - val_loss: 0.6908 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 348/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.5750 - 54ms/epoch - 18ms/step\n",
            "Epoch 349/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 350/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 351/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 352/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4565 - val_loss: 0.6908 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 353/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.5750 - 46ms/epoch - 15ms/step\n",
            "Epoch 354/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.6000 - 56ms/epoch - 19ms/step\n",
            "Epoch 355/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6908 - val_accuracy: 0.6000 - 40ms/epoch - 13ms/step\n",
            "Epoch 356/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 357/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 358/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 359/500\n",
            "3/3 - 0s - loss: 0.6935 - accuracy: 0.4674 - val_loss: 0.6908 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 360/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 361/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 362/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.6000 - 40ms/epoch - 13ms/step\n",
            "Epoch 363/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 364/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6908 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 365/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 366/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6908 - val_accuracy: 0.6000 - 42ms/epoch - 14ms/step\n",
            "Epoch 367/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 368/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 41ms/epoch - 14ms/step\n",
            "Epoch 369/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.6000 - 34ms/epoch - 11ms/step\n",
            "Epoch 370/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 371/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 372/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6908 - val_accuracy: 0.6000 - 40ms/epoch - 13ms/step\n",
            "Epoch 373/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6907 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 374/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6907 - val_accuracy: 0.5750 - 40ms/epoch - 13ms/step\n",
            "Epoch 375/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6907 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 376/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.5750 - 35ms/epoch - 12ms/step\n",
            "Epoch 377/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6907 - val_accuracy: 0.5750 - 44ms/epoch - 15ms/step\n",
            "Epoch 378/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4457 - val_loss: 0.6907 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 379/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4565 - val_loss: 0.6907 - val_accuracy: 0.5750 - 37ms/epoch - 12ms/step\n",
            "Epoch 380/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 43ms/epoch - 14ms/step\n",
            "Epoch 381/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 382/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 383/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4348 - val_loss: 0.6907 - val_accuracy: 0.5750 - 51ms/epoch - 17ms/step\n",
            "Epoch 384/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 385/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 386/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 387/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 388/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4348 - val_loss: 0.6907 - val_accuracy: 0.6000 - 51ms/epoch - 17ms/step\n",
            "Epoch 389/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 47ms/epoch - 16ms/step\n",
            "Epoch 390/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 391/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 392/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 393/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 394/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6907 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 395/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 396/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4565 - val_loss: 0.6907 - val_accuracy: 0.6250 - 35ms/epoch - 12ms/step\n",
            "Epoch 397/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4891 - val_loss: 0.6907 - val_accuracy: 0.6000 - 44ms/epoch - 15ms/step\n",
            "Epoch 398/500\n",
            "3/3 - 0s - loss: 0.6934 - accuracy: 0.4565 - val_loss: 0.6907 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 399/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 400/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6907 - val_accuracy: 0.6000 - 41ms/epoch - 14ms/step\n",
            "Epoch 401/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.5750 - 43ms/epoch - 14ms/step\n",
            "Epoch 402/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.6000 - 47ms/epoch - 16ms/step\n",
            "Epoch 403/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4565 - val_loss: 0.6907 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 404/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4565 - val_loss: 0.6907 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 405/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6907 - val_accuracy: 0.5750 - 36ms/epoch - 12ms/step\n",
            "Epoch 406/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4565 - val_loss: 0.6906 - val_accuracy: 0.6000 - 41ms/epoch - 14ms/step\n",
            "Epoch 407/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 54ms/epoch - 18ms/step\n",
            "Epoch 408/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 409/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6906 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 410/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 411/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 412/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 43ms/epoch - 14ms/step\n",
            "Epoch 413/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 414/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 415/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4565 - val_loss: 0.6906 - val_accuracy: 0.6000 - 40ms/epoch - 13ms/step\n",
            "Epoch 416/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 45ms/epoch - 15ms/step\n",
            "Epoch 417/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 418/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 419/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6906 - val_accuracy: 0.6000 - 44ms/epoch - 15ms/step\n",
            "Epoch 420/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4457 - val_loss: 0.6906 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 421/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4565 - val_loss: 0.6906 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 422/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 423/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 424/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 425/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4130 - val_loss: 0.6906 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 426/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6000 - 54ms/epoch - 18ms/step\n",
            "Epoch 427/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4457 - val_loss: 0.6906 - val_accuracy: 0.6250 - 39ms/epoch - 13ms/step\n",
            "Epoch 428/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6250 - 55ms/epoch - 18ms/step\n",
            "Epoch 429/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 430/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6250 - 36ms/epoch - 12ms/step\n",
            "Epoch 431/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 432/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 433/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6906 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 434/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6250 - 43ms/epoch - 14ms/step\n",
            "Epoch 435/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4674 - val_loss: 0.6906 - val_accuracy: 0.6250 - 39ms/epoch - 13ms/step\n",
            "Epoch 436/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4457 - val_loss: 0.6906 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 437/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6906 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 438/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6906 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 439/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6906 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 440/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6906 - val_accuracy: 0.6000 - 44ms/epoch - 15ms/step\n",
            "Epoch 441/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6906 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 442/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6906 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 443/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.5109 - val_loss: 0.6906 - val_accuracy: 0.6250 - 35ms/epoch - 12ms/step\n",
            "Epoch 444/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.4783 - val_loss: 0.6905 - val_accuracy: 0.6250 - 39ms/epoch - 13ms/step\n",
            "Epoch 445/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 40ms/epoch - 13ms/step\n",
            "Epoch 446/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6000 - 49ms/epoch - 16ms/step\n",
            "Epoch 447/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6000 - 43ms/epoch - 14ms/step\n",
            "Epoch 448/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 449/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 450/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 451/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6905 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 452/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6000 - 35ms/epoch - 12ms/step\n",
            "Epoch 453/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 454/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5217 - val_loss: 0.6905 - val_accuracy: 0.6250 - 36ms/epoch - 12ms/step\n",
            "Epoch 455/500\n",
            "3/3 - 0s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 36ms/epoch - 12ms/step\n",
            "Epoch 456/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6905 - val_accuracy: 0.6250 - 38ms/epoch - 13ms/step\n",
            "Epoch 457/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5326 - val_loss: 0.6905 - val_accuracy: 0.6250 - 47ms/epoch - 16ms/step\n",
            "Epoch 458/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 34ms/epoch - 11ms/step\n",
            "Epoch 459/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 460/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 461/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 45ms/epoch - 15ms/step\n",
            "Epoch 462/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6905 - val_accuracy: 0.6250 - 52ms/epoch - 17ms/step\n",
            "Epoch 463/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 42ms/epoch - 14ms/step\n",
            "Epoch 464/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4674 - val_loss: 0.6905 - val_accuracy: 0.6250 - 32ms/epoch - 11ms/step\n",
            "Epoch 465/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4783 - val_loss: 0.6905 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 466/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6905 - val_accuracy: 0.6250 - 36ms/epoch - 12ms/step\n",
            "Epoch 467/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5217 - val_loss: 0.6905 - val_accuracy: 0.6250 - 39ms/epoch - 13ms/step\n",
            "Epoch 468/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 54ms/epoch - 18ms/step\n",
            "Epoch 469/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 470/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 35ms/epoch - 12ms/step\n",
            "Epoch 471/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6905 - val_accuracy: 0.6250 - 36ms/epoch - 12ms/step\n",
            "Epoch 472/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6905 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 473/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 35ms/epoch - 12ms/step\n",
            "Epoch 474/500\n",
            "3/3 - 0s - loss: 0.6933 - accuracy: 0.4457 - val_loss: 0.6905 - val_accuracy: 0.6000 - 39ms/epoch - 13ms/step\n",
            "Epoch 475/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6905 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 476/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5217 - val_loss: 0.6905 - val_accuracy: 0.6250 - 37ms/epoch - 12ms/step\n",
            "Epoch 477/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 38ms/epoch - 13ms/step\n",
            "Epoch 478/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 39ms/epoch - 13ms/step\n",
            "Epoch 479/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 41ms/epoch - 14ms/step\n",
            "Epoch 480/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 39ms/epoch - 13ms/step\n",
            "Epoch 481/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.6250 - 38ms/epoch - 13ms/step\n",
            "Epoch 482/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6905 - val_accuracy: 0.6250 - 35ms/epoch - 12ms/step\n",
            "Epoch 483/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4891 - val_loss: 0.6905 - val_accuracy: 0.6250 - 44ms/epoch - 15ms/step\n",
            "Epoch 484/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4783 - val_loss: 0.6905 - val_accuracy: 0.6250 - 38ms/epoch - 13ms/step\n",
            "Epoch 485/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4783 - val_loss: 0.6905 - val_accuracy: 0.6250 - 40ms/epoch - 13ms/step\n",
            "Epoch 486/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 487/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5217 - val_loss: 0.6904 - val_accuracy: 0.6250 - 45ms/epoch - 15ms/step\n",
            "Epoch 488/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6250 - 40ms/epoch - 13ms/step\n",
            "Epoch 489/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4783 - val_loss: 0.6904 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 490/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4783 - val_loss: 0.6904 - val_accuracy: 0.6000 - 40ms/epoch - 13ms/step\n",
            "Epoch 491/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6000 - 37ms/epoch - 12ms/step\n",
            "Epoch 492/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6000 - 36ms/epoch - 12ms/step\n",
            "Epoch 493/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5326 - val_loss: 0.6904 - val_accuracy: 0.6250 - 44ms/epoch - 15ms/step\n",
            "Epoch 494/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6250 - 40ms/epoch - 13ms/step\n",
            "Epoch 495/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.4891 - val_loss: 0.6904 - val_accuracy: 0.6250 - 54ms/epoch - 18ms/step\n",
            "Epoch 496/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.4891 - val_loss: 0.6904 - val_accuracy: 0.6000 - 38ms/epoch - 13ms/step\n",
            "Epoch 497/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6250 - 42ms/epoch - 14ms/step\n",
            "Epoch 498/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5217 - val_loss: 0.6904 - val_accuracy: 0.6250 - 40ms/epoch - 13ms/step\n",
            "Epoch 499/500\n",
            "3/3 - 0s - loss: 0.6930 - accuracy: 0.5326 - val_loss: 0.6904 - val_accuracy: 0.6250 - 55ms/epoch - 18ms/step\n",
            "Epoch 500/500\n",
            "3/3 - 0s - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6904 - val_accuracy: 0.6000 - 31ms/epoch - 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIo_BwY_jdgd"
      },
      "source": [
        "**Optimized Variable Values:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTAHH0itjdge",
        "outputId": "259f848c-90bc-4aeb-b0af-f77d1ffd15dc"
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.05680541, -0.18841583, -0.0963508 , -0.05300212,  0.09615692,\n",
              "         -0.09823869,  0.13921754,  0.36713368, -0.1913799 , -0.12987858,\n",
              "          0.3774119 ,  0.04249936,  0.07989561, -0.33629683, -0.0970625 ],\n",
              "        [ 0.14084613,  0.3726643 ,  0.2496718 , -0.13569427,  0.35888582,\n",
              "         -0.28251114,  0.38242462,  0.3780955 , -0.11667102,  0.3158068 ,\n",
              "         -0.15779112, -0.00749749,  0.07500647,  0.07216553, -0.18047513],\n",
              "        [ 0.28839788, -0.05896486,  0.00900921,  0.15449953,  0.27341   ,\n",
              "         -0.33062282,  0.08761822,  0.1350745 , -0.37706843,  0.13391566,\n",
              "          0.09212138,  0.05614209,  0.04670829,  0.35307372,  0.05999343],\n",
              "        [-0.32115954,  0.1796914 , -0.34467578,  0.12658934,  0.12118852,\n",
              "          0.27941808, -0.31558216, -0.19778866, -0.03081416,  0.20653576,\n",
              "          0.08316062, -0.32285228,  0.13273036,  0.3311531 , -0.0490044 ],\n",
              "        [-0.37613437,  0.07902863,  0.27350312,  0.33559632, -0.09231867,\n",
              "          0.08736683,  0.18242593, -0.00329594, -0.28455317, -0.3295018 ,\n",
              "          0.30687878, -0.29338735, -0.17582841,  0.19674525,  0.12548925],\n",
              "        [-0.21201661,  0.17192112, -0.24392104,  0.33558068,  0.13521793,\n",
              "         -0.24978139, -0.22116823,  0.06691279,  0.36925796, -0.2856143 ,\n",
              "         -0.37010363, -0.2933925 , -0.10313663, -0.3830698 ,  0.05165609],\n",
              "        [ 0.3179518 ,  0.34213147,  0.12080622, -0.21232985,  0.31989482,\n",
              "         -0.17449144, -0.22810772, -0.3329547 ,  0.23813055, -0.32994154,\n",
              "          0.09717316, -0.11142626, -0.31233257, -0.02537986,  0.3109416 ],\n",
              "        [ 0.350456  , -0.04298129, -0.22002015,  0.17308876,  0.14460227,\n",
              "         -0.11572936, -0.00340243,  0.1978892 ,  0.37047234,  0.27573848,\n",
              "         -0.00621741, -0.28490922, -0.04074902,  0.17752932, -0.3140416 ],\n",
              "        [-0.31132045, -0.0251488 ,  0.37058082,  0.03499186, -0.36901662,\n",
              "          0.30483925, -0.31055492, -0.351865  , -0.31888488, -0.17001432,\n",
              "          0.35864338, -0.3400451 , -0.31380525,  0.01431591, -0.24140382],\n",
              "        [-0.28671557, -0.37300506, -0.35251826, -0.23169337, -0.33127078,\n",
              "          0.02117324,  0.0141016 , -0.11429508,  0.31399363,  0.10599211,\n",
              "          0.23504992, -0.05869842, -0.36910424, -0.03263075,  0.3380622 ],\n",
              "        [-0.36809322,  0.29309112,  0.29321238, -0.31917295, -0.37944096,\n",
              "         -0.2614502 , -0.06461336,  0.3543907 , -0.29396772,  0.14419878,\n",
              "         -0.20958118,  0.35974395, -0.19120964,  0.25292164, -0.33626676],\n",
              "        [ 0.20183364,  0.22409219,  0.17557414,  0.36755905, -0.18156898,\n",
              "         -0.35123163, -0.21700433,  0.08818007, -0.2249109 , -0.37122026,\n",
              "          0.19661115, -0.3374042 , -0.35274002,  0.05692044,  0.37197384],\n",
              "        [-0.22318144, -0.00976137,  0.3109903 , -0.10580714,  0.26505023,\n",
              "          0.10620173, -0.19986609, -0.08154932, -0.37527972,  0.10446677,\n",
              "         -0.31805316, -0.0703145 ,  0.30291033, -0.01106552,  0.05998826],\n",
              "        [-0.19529101, -0.04506065, -0.04198658,  0.28175902, -0.31957263,\n",
              "          0.17157294, -0.10224465, -0.25144994,  0.2669999 ,  0.0217573 ,\n",
              "          0.08972403, -0.27258682,  0.29942515, -0.06283981, -0.02177206],\n",
              "        [-0.2738856 , -0.33616826, -0.32502252,  0.03161317,  0.37282237,\n",
              "          0.10701582, -0.03558895,  0.12407498, -0.21254043, -0.27978745,\n",
              "         -0.08316444,  0.20554161,  0.09749508,  0.1826731 ,  0.13874057],\n",
              "        [ 0.16470474,  0.17714193, -0.16530861, -0.34863168,  0.2245842 ,\n",
              "          0.334943  ,  0.20042086,  0.14791073, -0.27337766, -0.1322273 ,\n",
              "          0.05969912, -0.22628902,  0.08644219, -0.19704336, -0.17071432],\n",
              "        [ 0.38554803,  0.17789535, -0.22719838,  0.04128169, -0.08582968,\n",
              "          0.36274213, -0.1321537 ,  0.37143454,  0.37645915,  0.06965694,\n",
              "          0.04464482, -0.38535434,  0.13089468, -0.001906  , -0.07565532],\n",
              "        [ 0.06149925, -0.39106768,  0.13583335, -0.12632872,  0.03578592,\n",
              "          0.35137668, -0.24226655, -0.36685064,  0.01718034, -0.14971754,\n",
              "          0.04611721, -0.3394711 ,  0.38640568, -0.32967132,  0.24081787],\n",
              "        [-0.03999412, -0.17003557,  0.055328  , -0.36772433, -0.3808168 ,\n",
              "         -0.3291436 , -0.208992  , -0.3605542 , -0.13951153, -0.22986725,\n",
              "         -0.28899708, -0.12239051, -0.17497759,  0.3770117 ,  0.3715597 ],\n",
              "        [ 0.10170049, -0.22162266, -0.36295083, -0.28881434, -0.3241506 ,\n",
              "         -0.04407408, -0.33143112, -0.10527211, -0.10730123, -0.02677566,\n",
              "         -0.13580498,  0.17803001, -0.20162626, -0.32812706,  0.3379306 ],\n",
              "        [ 0.2174116 ,  0.01826715,  0.37474337, -0.04509315,  0.09637257,\n",
              "         -0.20432433, -0.37811026, -0.2199601 ,  0.05616728,  0.3850944 ,\n",
              "          0.04297534, -0.11646309, -0.268936  , -0.35172942,  0.21624096],\n",
              "        [ 0.36565748, -0.18135388, -0.19425285, -0.31767687, -0.32209808,\n",
              "         -0.287573  ,  0.3263312 , -0.09671564,  0.03054301, -0.13243192,\n",
              "          0.24524648, -0.25069296, -0.32049942,  0.02692663, -0.32910535],\n",
              "        [-0.16348332, -0.09233019,  0.0462369 ,  0.11722529,  0.14659415,\n",
              "         -0.21180369,  0.33473772, -0.09542559,  0.1131694 , -0.30041572,\n",
              "          0.08602022,  0.33254373,  0.15111649,  0.18677686, -0.16419926],\n",
              "        [-0.06513777, -0.16578706,  0.33233383,  0.05178659, -0.10676464,\n",
              "          0.2542041 ,  0.25578704, -0.02356199,  0.4046512 , -0.35366142,\n",
              "         -0.20147687, -0.3425437 ,  0.04841796,  0.3138441 , -0.36970812],\n",
              "        [ 0.35300684,  0.15370458,  0.16683634, -0.3025905 ,  0.254645  ,\n",
              "         -0.3189966 ,  0.37626714,  0.1517231 ,  0.03220508, -0.23563826,\n",
              "         -0.18158378, -0.19701567, -0.21441856,  0.16513401, -0.20422603]],\n",
              "       dtype=float32),\n",
              " array([ 0.0002451 , -0.04547311,  0.01803044, -0.00084898,  0.00200536,\n",
              "        -0.01384524, -0.00222384,  0.00028429, -0.00169733,  0.        ,\n",
              "        -0.00532798,  0.        ,  0.00254502, -0.01677697, -0.0021113 ],\n",
              "       dtype=float32),\n",
              " array([[ 0.01401933,  0.43002108, -0.10144714,  0.12344287, -0.195933  ,\n",
              "         -0.27728602, -0.31424692, -0.4649475 ],\n",
              "        [-0.17345214,  0.4944975 ,  0.42839575,  0.34716827,  0.29621595,\n",
              "          0.37671715,  0.35139   , -0.07821497],\n",
              "        [ 0.28659204, -0.35470393, -0.25074062,  0.40183386, -0.27589354,\n",
              "         -0.12743443, -0.12356098, -0.22804928],\n",
              "        [ 0.16650745,  0.18562202,  0.28044865,  0.08533552, -0.29901376,\n",
              "          0.05015544,  0.34319326, -0.15053281],\n",
              "        [-0.28435034, -0.01546471,  0.16354854,  0.15528357, -0.49879774,\n",
              "         -0.12772727, -0.4563001 ,  0.22370565],\n",
              "        [ 0.12487499, -0.4556682 , -0.42944026, -0.3906838 ,  0.38096586,\n",
              "          0.06123791,  0.40423816, -0.05979021],\n",
              "        [-0.45026216, -0.34839773,  0.2834642 , -0.44246858, -0.03892603,\n",
              "         -0.43140468, -0.45830667,  0.29291037],\n",
              "        [-0.36303824,  0.17666085, -0.2784993 , -0.14913923, -0.05806545,\n",
              "         -0.33348164, -0.1960664 , -0.22212619],\n",
              "        [-0.45851937,  0.38840723,  0.12449211, -0.480439  , -0.2764058 ,\n",
              "         -0.11350904,  0.29373944, -0.10592832],\n",
              "        [ 0.0645414 , -0.46520537,  0.00852472,  0.21081394, -0.25323328,\n",
              "          0.20926851, -0.30528852,  0.4076271 ],\n",
              "        [ 0.45159474,  0.29746222,  0.06487076, -0.2410607 , -0.15806983,\n",
              "         -0.3459093 , -0.27667934, -0.41892913],\n",
              "        [-0.07744679,  0.48465246, -0.2484208 ,  0.30543292, -0.42438394,\n",
              "         -0.16355923,  0.22393245,  0.28014177],\n",
              "        [ 0.5096903 , -0.24729912,  0.3371262 ,  0.15547211, -0.25475645,\n",
              "          0.17478395,  0.11148801,  0.47365853],\n",
              "        [ 0.30734864, -0.10240564, -0.20969601,  0.40517303, -0.21109565,\n",
              "          0.48794934, -0.25306657, -0.21141572],\n",
              "        [-0.48168895, -0.04840505,  0.32962853, -0.0731314 , -0.09058444,\n",
              "         -0.39500502,  0.19576861,  0.11612227]], dtype=float32),\n",
              " array([ 0.00086632, -0.00148823, -0.00051029,  0.00049168,  0.00024971,\n",
              "        -0.00047309, -0.00126521,  0.0005422 ], dtype=float32),\n",
              " array([[ 0.41177696, -0.21854733,  0.40679365],\n",
              "        [ 0.22023864,  0.55642366, -0.65692335],\n",
              "        [-0.68211466,  0.61287355, -0.24415962],\n",
              "        [ 0.6843891 , -0.3750201 ,  0.05692162],\n",
              "        [ 0.38653186, -0.6238682 , -0.44308612],\n",
              "        [-0.5066764 ,  0.24018514, -0.20722753],\n",
              "        [ 0.492635  ,  0.46620017, -0.4508599 ],\n",
              "        [-0.38422355, -0.73324263, -0.7072585 ]], dtype=float32),\n",
              " array([-0.00151758, -0.00636459,  0.00388364], dtype=float32),\n",
              " array([[ 0.08122294],\n",
              "        [ 0.4092216 ],\n",
              "        [-0.85872024]], dtype=float32),\n",
              " array([-0.05982401], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-OYyiHSjdge"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "XIu4OuHdjdgf",
        "outputId": "bec6ab66-784c-44c7-e89b-7771cb046454"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7WDfelMjdgf",
        "outputId": "abaeba2c-90f0-494d-ec59-e316f979615e"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "y_pred_prob = model.predict(X_test)\n",
        "log_loss(y_true=y_test,y_pred=y_pred_prob)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6904182597994805"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2BSdIdLjdgf",
        "outputId": "7cb7ac26-3ab6-4dd7-fc02-7e871e6270e5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predict_probs= model.predict(X_test)\n",
        "predict_probs[:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5041428 ],\n",
              "       [0.5052107 ],\n",
              "       [0.49846953],\n",
              "       [0.4843429 ],\n",
              "       [0.49256948]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ4gWVKZkqEt",
        "outputId": "1e8bfd73-ce0a-45a9-d3e8-36d2c3cba773"
      },
      "source": [
        "predict_classes = np.where(predict_probs>=0.5,1,0)\n",
        "predict_classes[:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpeU5c0RkjUC",
        "outputId": "883c7e37-86a1-48f9-c187-4ae2a49da38a"
      },
      "source": [
        "acc = accuracy_score(y_test,predict_classes)\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut5cZlQOjdgg"
      },
      "source": [
        "# Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM5lrtT5jdgg"
      },
      "source": [
        "If we continue to train the network, it may overfit. We can have a stop point where the accuracy may start falling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Scvd4rjdgg"
      },
      "source": [
        "For getting reproducible results, we set random number seed and do necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBTjV41ejdgh"
      },
      "source": [
        "tf.random.set_seed(2021)\n",
        "model = tf.keras.models.Sequential([ \n",
        "    tf.keras.layers.Dense(15, activation='relu',input_shape=(X_train.shape[1], )), \n",
        "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(3, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')   \n",
        "])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJmntl6Tjdgh"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5lU7KCLjdgh",
        "outputId": "10c1032e-6ad2-4b7c-abbf-9964e334ab1c"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=2, mode='auto',\n",
        "        restore_best_weights=True)\n",
        "history2 = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=[monitor],verbose=2,epochs=500)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 - 1s - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000 - 678ms/epoch - 226ms/step\n",
            "Epoch 2/500\n",
            "3/3 - 0s - loss: 0.6960 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000 - 33ms/epoch - 11ms/step\n",
            "Epoch 3/500\n",
            "3/3 - 0s - loss: 0.6960 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000 - 32ms/epoch - 11ms/step\n",
            "Epoch 4/500\n",
            "3/3 - 0s - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 32ms/epoch - 11ms/step\n",
            "Epoch 5/500\n",
            "3/3 - 0s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 35ms/epoch - 12ms/step\n",
            "Epoch 6/500\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "3/3 - 0s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 36ms/epoch - 12ms/step\n",
            "Epoch 00006: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hu2pXPHjdgi"
      },
      "source": [
        "Let us see the parameters in the early stopping involved:\n",
        "- **monitor**: quantity to be monitored.<br>\n",
        "- **min_delta**: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.<br>\n",
        "- **patience**: number of epochs with no improvement after which training will be stopped.<br>\n",
        "- **mode**: one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n",
        "- **baseline**: Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline.<br>\n",
        "- **restore_best_weights**: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3RFagZHjdgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b63d6722-eb7c-4e59-f30b-525d51e3a2d0"
      },
      "source": [
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0CQ3erljdgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e98955-f820-40a1-b3e0-650ab1c23858"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "y_pred_prob = model.predict(X_test)\n",
        "log_loss(y_true=y_test,y_pred=y_pred_prob)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6933561965823174"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOnFILUrjdgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea5279d-b4bc-4e58-f95c-fc4db02a4c77"
      },
      "source": [
        "predict_probs= model.predict(X_test)\n",
        "predict_probs[:5]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5291076 ],\n",
              "       [0.5288677 ],\n",
              "       [0.5238184 ],\n",
              "       [0.51262766],\n",
              "       [0.5246466 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_QwhncitAST",
        "outputId": "e43f6239-0974-4a90-f0cd-b0389f317e35"
      },
      "source": [
        "predict_classes = np.where(predict_probs>=0.5,1,0)\n",
        "predict_classes[:5]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnI4I95vtGmy",
        "outputId": "732c727b-5c69-4625-8ed2-eba4d3c632c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acc = accuracy_score(y_test,predict_classes)\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}